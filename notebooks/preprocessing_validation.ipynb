{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Validation - Missing Value Handling\n",
    "\n",
    "This notebook validates the missing value handling strategy implemented in `src/data/preprocessing.py`.\n",
    "\n",
    "**Objectives:**\n",
    "- Compare dataset characteristics before and after preprocessing\n",
    "- Verify that missing values were handled correctly\n",
    "- Check that data distributions were preserved during imputation\n",
    "- Validate that no data quality issues were introduced\n",
    "\n",
    "**Files analyzed:**\n",
    "- Input: `openfoodfacts_filtered.csv` (with missing values)\n",
    "- Output: `openfoodfacts_preprocessed.csv` (after handling missing values)\n",
    "- Report: `imputation_report.json` (detailed documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd           # Data manipulation and analysis\n",
    "import numpy as np             # Numerical operations\n",
    "import matplotlib.pyplot as plt # Plotting\n",
    "import seaborn as sns          # Statistical visualizations\n",
    "import json                    # Reading JSON report\n",
    "\n",
    "# Configure visualization style for cleaner plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)  # Default figure size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Load both the original filtered dataset and the preprocessed dataset to compare them. Also load the JSON report that documents all imputation decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset BEFORE preprocessing (contains missing values)\n",
    "df_before = pd.read_csv('../data/processed/openfoodfacts_filtered.csv')\n",
    "\n",
    "# Load the dataset AFTER preprocessing (missing values handled)\n",
    "df_after = pd.read_csv('../data/processed/openfoodfacts_preprocessed.csv')\n",
    "\n",
    "# Load the imputation report (contains strategy and statistics)\n",
    "with open('../data/processed/imputation_report.json', 'r') as f:\n",
    "    imputation_report = json.load(f)\n",
    "\n",
    "# Display basic information about dataset dimensions\n",
    "print(f\"Before preprocessing: {df_before.shape}\")\n",
    "print(f\"After preprocessing: {df_after.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Values Comparison\n",
    "\n",
    "Calculate and compare missing value counts and percentages for each feature before and after preprocessing. This helps verify that the preprocessing step successfully addressed all missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values in each dataset\n",
    "missing_before = df_before.isnull().sum()  # Number of missing values per column (before)\n",
    "missing_after = df_after.isnull().sum()     # Number of missing values per column (after)\n",
    "\n",
    "# Create a comparison table with counts and percentages\n",
    "comparison = pd.DataFrame({\n",
    "    'Before (count)': missing_before,\n",
    "    'Before (%)': (missing_before / len(df_before)) * 100,\n",
    "    'After (count)': missing_after if len(missing_after) > 0 else 0,\n",
    "    'After (%)': (missing_after / len(df_after)) * 100 if len(missing_after) > 0 else 0\n",
    "})\n",
    "\n",
    "# Filter to show only columns that had missing values before preprocessing\n",
    "# Sort by missing percentage (descending) to see worst cases first\n",
    "comparison_filtered = comparison[comparison['Before (count)'] > 0].sort_values('Before (%)', ascending=False)\n",
    "\n",
    "print(\"\\nMissing Values Comparison:\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side bar charts to visualize missing values before and after\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# LEFT PLOT: Missing values BEFORE preprocessing\n",
    "missing_pct_before = (missing_before / len(df_before)) * 100\n",
    "missing_pct_before = missing_pct_before[missing_pct_before > 0].sort_values(ascending=True)\n",
    "missing_pct_before.plot(kind='barh', ax=axes[0], color='coral')\n",
    "axes[0].set_title('Missing Values BEFORE Preprocessing', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Missing Percentage (%)')\n",
    "# Add red line at 95% threshold (features above this were dropped)\n",
    "axes[0].axvline(95, color='red', linestyle='--', linewidth=2, label='95% threshold')\n",
    "axes[0].legend()\n",
    "\n",
    "# RIGHT PLOT: Missing values AFTER preprocessing\n",
    "if df_after.isnull().sum().sum() > 0:\n",
    "    # If there are still missing values, plot them\n",
    "    missing_pct_after = (missing_after / len(df_after)) * 100\n",
    "    missing_pct_after = missing_pct_after[missing_pct_after > 0].sort_values(ascending=True)\n",
    "    missing_pct_after.plot(kind='barh', ax=axes[1], color='lightgreen')\n",
    "    axes[1].set_title('Missing Values AFTER Preprocessing', fontsize=14, fontweight='bold')\n",
    "else:\n",
    "    # If no missing values remain, show success message\n",
    "    axes[1].text(0.5, 0.5, '✓ No Missing Values!', \n",
    "                ha='center', va='center', fontsize=20, color='green', fontweight='bold')\n",
    "    axes[1].set_title('Missing Values AFTER Preprocessing', fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Imputation Statistics\n",
    "\n",
    "Display the detailed imputation strategy used for each feature. The report shows:\n",
    "- **Method**: The imputation technique (median, constant, etc.)\n",
    "- **Value**: The actual value used for imputation\n",
    "- **Missing count**: How many values were imputed\n",
    "- **Rationale**: Why this strategy was chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract imputation statistics from the JSON report\n",
    "# This dictionary contains details about how each feature was imputed\n",
    "imputation_stats = imputation_report['imputation_statistics']\n",
    "\n",
    "print(\"\\nImputation Strategy Summary:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Iterate through each imputed feature and display its strategy\n",
    "for col, stats in imputation_stats.items():\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Method: {stats['method']}\")           # Technique used (median, constant)\n",
    "    print(f\"  Value: {stats['value']}\")              # Imputation value\n",
    "    print(f\"  Missing count: {stats['missing_count']:,}\")  # Number of values filled\n",
    "    print(f\"  Rationale: {stats['rationale']}\")     # Explanation for strategy choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distribution Comparison (Key Features)\n",
    "\n",
    "Verify that imputation preserved the original data distributions. This is important because:\n",
    "- Median imputation should not significantly alter the distribution shape\n",
    "- If distributions change dramatically, it may indicate imputation introduced bias\n",
    "\n",
    "We compare histograms of features before (excluding missing) and after (with imputed values) preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key features that had significant missing values for comparison\n",
    "key_features = ['fiber_100g', 'additives_n', 'fat_100g', 'sugars_100g']\n",
    "\n",
    "# Create a grid of subplots: 2 plots per feature (before and after)\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()  # Flatten to 1D array for easier indexing\n",
    "\n",
    "# Generate comparison histograms for each feature\n",
    "for idx, feature in enumerate(key_features):\n",
    "    if feature in df_before.columns and feature in df_after.columns:\n",
    "        \n",
    "        # BEFORE: Get non-missing values only\n",
    "        data_before = df_before[feature].dropna()\n",
    "        \n",
    "        # Filter extreme outliers using 99th percentile for better visualization\n",
    "        q99 = data_before.quantile(0.99)\n",
    "        data_before_filtered = data_before[data_before <= q99]\n",
    "        \n",
    "        # Plot BEFORE distribution (left plot for each feature)\n",
    "        data_before_filtered.hist(bins=50, ax=axes[idx*2], alpha=0.7, color='coral', edgecolor='black')\n",
    "        axes[idx*2].set_title(f'{feature} - BEFORE\\n(excluding missing)', fontweight='bold')\n",
    "        axes[idx*2].set_ylabel('Frequency')\n",
    "        # Show median line\n",
    "        axes[idx*2].axvline(data_before.median(), color='red', linestyle='--', \n",
    "                           label=f'Median: {data_before.median():.2f}')\n",
    "        axes[idx*2].legend()\n",
    "        \n",
    "        # AFTER: Get all values (including imputed)\n",
    "        data_after = df_after[feature]\n",
    "        data_after_filtered = data_after[data_after <= q99]\n",
    "        \n",
    "        # Plot AFTER distribution (right plot for each feature)\n",
    "        data_after_filtered.hist(bins=50, ax=axes[idx*2+1], alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "        axes[idx*2+1].set_title(f'{feature} - AFTER\\n(with imputation)', fontweight='bold')\n",
    "        # Show median line\n",
    "        axes[idx*2+1].axvline(data_after.median(), color='red', linestyle='--', \n",
    "                             label=f'Median: {data_after.median():.2f}')\n",
    "        axes[idx*2+1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dropped Features\n",
    "\n",
    "List features that were dropped due to excessive missing values (>95% threshold). Features with very high missing rates provide little useful information and can be safely removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract list of dropped features from the report\n",
    "dropped_features = imputation_report['dropped_features']\n",
    "\n",
    "print(\"\\nDropped Features (>95% missing):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Display each dropped feature with its missing percentage\n",
    "for feature in dropped_features:\n",
    "    missing_info = imputation_report['missing_value_analysis'][feature]\n",
    "    print(f\"  - {feature:45s} ({missing_info['missing_percentage']:.2f}% missing)\")\n",
    "\n",
    "print(f\"\\nTotal features dropped: {len(dropped_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Validation\n",
    "\n",
    "Perform final checks to ensure preprocessing was successful:\n",
    "- Verify data retention (no unexpected row loss)\n",
    "- Confirm all missing values were eliminated\n",
    "- Check that target variable has no missing values\n",
    "- Validate data types are appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform comprehensive validation checks\n",
    "print(\"\\nFinal Validation:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check 1: Data retention - verify we didn't lose rows unexpectedly\n",
    "print(f\"✓ Rows retained: {len(df_after):,} / {len(df_before):,} ({len(df_after)/len(df_before)*100:.2f}%)\")\n",
    "\n",
    "# Check 2: Column count - some may be dropped due to high missing rate\n",
    "print(f\"✓ Columns retained: {len(df_after.columns)} / {len(df_before.columns)}\")\n",
    "\n",
    "# Check 3: Missing values - should be zero\n",
    "print(f\"✓ Missing values remaining: {df_after.isnull().sum().sum()}\")\n",
    "\n",
    "# Check 4: Target variable - must have no missing values for modeling\n",
    "print(f\"✓ Target variable complete: {df_after['nutriscore_grade'].notna().all()}\")\n",
    "\n",
    "# Display data types to ensure they are appropriate\n",
    "print(\"\\nData Types:\")\n",
    "print(df_after.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Strategy Applied\n",
    "\n",
    "Based on missing percentage thresholds, the following approaches were used:\n",
    "\n",
    "1. **High missing rate (>95%)**: Dropped 2 columns (fruits-vegetables-nuts_100g, fruits-vegetables-nuts-estimate_100g) as they provide insufficient information\n",
    "\n",
    "2. **Numerical features**: \n",
    "   - Nutritional values (energy, fat, proteins, etc.): Median imputation to preserve distribution\n",
    "   - additives_n: Filled with 0 (reasonable assumption: if not listed, likely no additives)\n",
    "\n",
    "3. **Categorical features**: Filled with 'unknown' placeholder (brands, product_name, categories, countries)\n",
    "\n",
    "4. **Data retention**: All 100,000 rows preserved (0% data loss)\n",
    "\n",
    "### Validation Results\n",
    "\n",
    "- Final dataset: 100,000 rows × 20 columns\n",
    "- Missing values eliminated: 0 remaining\n",
    "- Distribution characteristics maintained (verified via histograms)\n",
    "- Dataset ready for next preprocessing steps (outlier removal, feature engineering)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
